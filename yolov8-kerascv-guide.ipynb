{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/arielguerchenzon/yolov8-kerascv-guide?scriptVersionId=159116305\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# This is my attempt at creating a YoloV8 model for object detection\n\nIt will be completely based in this tutorial: https://keras.io/examples/vision/yolov8/","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/keras-team/keras-cv -q","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:07:36.898885Z","iopub.execute_input":"2024-01-16T01:07:36.899324Z","iopub.status.idle":"2024-01-16T01:08:08.992228Z","shell.execute_reply.started":"2024-01-16T01:07:36.899292Z","shell.execute_reply":"2024-01-16T01:08:08.990752Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nfrom tqdm.auto import tqdm\nimport xml.etree.ElementTree as ET\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport keras_cv\nfrom keras_cv import bounding_box\nfrom keras_cv import visualization","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:08.995238Z","iopub.execute_input":"2024-01-16T01:08:08.995775Z","iopub.status.idle":"2024-01-16T01:08:09.004403Z","shell.execute_reply.started":"2024-01-16T01:08:08.995737Z","shell.execute_reply":"2024-01-16T01:08:09.00332Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Hiperparams","metadata":{"execution":{"iopub.status.busy":"2024-01-16T00:53:31.901324Z","iopub.execute_input":"2024-01-16T00:53:31.901781Z","iopub.status.idle":"2024-01-16T00:53:31.911925Z","shell.execute_reply.started":"2024-01-16T00:53:31.901745Z","shell.execute_reply":"2024-01-16T00:53:31.910719Z"}}},{"cell_type":"code","source":"SPLIT_RATIO = 0.2\nBATCH_SIZE = 4\nLEARNING_RATE = 0.001\nEPOCH = 5\nGLOBAL_CLIPNORM = 10.0","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.006513Z","iopub.execute_input":"2024-01-16T01:08:09.007007Z","iopub.status.idle":"2024-01-16T01:08:09.01994Z","shell.execute_reply.started":"2024-01-16T01:08:09.006965Z","shell.execute_reply":"2024-01-16T01:08:09.018997Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:16:04.87893Z","iopub.execute_input":"2024-01-16T01:16:04.87936Z","iopub.status.idle":"2024-01-16T01:16:04.885063Z","shell.execute_reply.started":"2024-01-16T01:16:04.879328Z","shell.execute_reply":"2024-01-16T01:16:04.883475Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class_ids = [\n    \"car\",\n]\nclass_mapping = dict(zip(range(len(class_ids)), class_ids))\n\n# Path to images and annotations\npath_images = \"/kaggle/input/car-object-detection/data/training_images/\"\nannot_file = \"/kaggle/input/car-object-detection/data/train_solution_bounding_boxes (1).csv\"\n\n# Get all JPEG image file paths in path_images and sort them\njpg_files = sorted(\n    [\n        os.path.join(path_images, file_name)\n        for file_name in os.listdir(path_images)\n        if file_name.endswith(\".jpg\")\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:32:04.154461Z","iopub.execute_input":"2024-01-16T01:32:04.154898Z","iopub.status.idle":"2024-01-16T01:32:04.167538Z","shell.execute_reply.started":"2024-01-16T01:32:04.154866Z","shell.execute_reply":"2024-01-16T01:32:04.166167Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(annot_file)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:30:27.389143Z","iopub.execute_input":"2024-01-16T01:30:27.389602Z","iopub.status.idle":"2024-01-16T01:30:27.401432Z","shell.execute_reply.started":"2024-01-16T01:30:27.389565Z","shell.execute_reply":"2024-01-16T01:30:27.400032Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"image_paths = jpg_files.copy()\nbbox = df.apply(lambda row: [row.xmin, row.ymin, row.xmax, row.ymax], axis=1)\nclasses = [0]*len(df)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:32:14.369771Z","iopub.execute_input":"2024-01-16T01:32:14.370317Z","iopub.status.idle":"2024-01-16T01:32:14.410759Z","shell.execute_reply.started":"2024-01-16T01:32:14.370276Z","shell.execute_reply":"2024-01-16T01:32:14.409621Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### TODO: Fix Tensor creation from Dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:34:11.106864Z","iopub.execute_input":"2024-01-16T01:34:11.107274Z","iopub.status.idle":"2024-01-16T01:34:11.245683Z","shell.execute_reply.started":"2024-01-16T01:34:11.107244Z","shell.execute_reply":"2024-01-16T01:34:11.244363Z"}}},{"cell_type":"code","source":"bbox = tf.ragged.constant(bbox)\nclasses = tf.ragged.constant(classes)\nimage_paths = tf.ragged.constant(image_paths)\n\ndata = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:30:45.675862Z","iopub.execute_input":"2024-01-16T01:30:45.676284Z","iopub.status.idle":"2024-01-16T01:30:45.801152Z","shell.execute_reply.started":"2024-01-16T01:30:45.676253Z","shell.execute_reply":"2024-01-16T01:30:45.799319Z"},"trusted":true},"execution_count":41,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m classes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mragged\u001b[38;5;241m.\u001b[39mconstant(classes)\n\u001b[1;32m      3\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mragged\u001b[38;5;241m.\u001b[39mconstant(image_paths)\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:831\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:45\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\n\u001b[1;32m     43\u001b[0m     tensor_shape\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 45\u001b[0m   \u001b[43mbatch_dim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mtensor_slice_dataset(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors,\n\u001b[1;32m     51\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure),\n\u001b[1;32m     52\u001b[0m     is_files\u001b[38;5;241m=\u001b[39mis_files,\n\u001b[1;32m     53\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mSerializeToString())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:300\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    is_compatible_with).\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m--> 300\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are not compatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    301\u001b[0m                    (\u001b[38;5;28mself\u001b[39m, other))\n","\u001b[0;31mValueError\u001b[0m: Dimensions 1001 and 559 are not compatible"],"ename":"ValueError","evalue":"Dimensions 1001 and 559 are not compatible","output_type":"error"}]},{"cell_type":"code","source":"# Determine the number of validation samples\nnum_val = int(len(xml_files) * SPLIT_RATIO)\n\n# Split the dataset into train and validation sets\nval_data = data.take(num_val)\ntrain_data = data.skip(num_val)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.115772Z","iopub.status.idle":"2024-01-16T01:08:09.116161Z","shell.execute_reply.started":"2024-01-16T01:08:09.115972Z","shell.execute_reply":"2024-01-16T01:08:09.115989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_data.take(1):\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.117365Z","iopub.status.idle":"2024-01-16T01:08:09.117754Z","shell.execute_reply.started":"2024-01-16T01:08:09.117565Z","shell.execute_reply":"2024-01-16T01:08:09.117582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bounding_boxes = {\n    # num_boxes may be a Ragged dimension\n    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n    'classes': Tensor(shape=[batch, num_boxes])\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.118657Z","iopub.status.idle":"2024-01-16T01:08:09.119112Z","shell.execute_reply.started":"2024-01-16T01:08:09.118848Z","shell.execute_reply":"2024-01-16T01:08:09.118865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    return image\n\n\ndef load_dataset(image_path, classes, bbox):\n    # Read Image\n    image = load_image(image_path)\n    bounding_boxes = {\n        \"classes\": tf.cast(classes, dtype=tf.float32),\n        \"boxes\": bbox,\n    }\n    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.120286Z","iopub.status.idle":"2024-01-16T01:08:09.120756Z","shell.execute_reply.started":"2024-01-16T01:08:09.120473Z","shell.execute_reply":"2024-01-16T01:08:09.120509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxes = keras_cv.bounding_box.convert_format(\n        bounding_box,\n        images=image,\n        source=\"xyxy\",  # Original Format\n        target=\"xywh\",  # Target Format (to which we want to convert)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.12257Z","iopub.status.idle":"2024-01-16T01:08:09.123152Z","shell.execute_reply.started":"2024-01-16T01:08:09.122839Z","shell.execute_reply":"2024-01-16T01:08:09.122864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data augmentation","metadata":{}},{"cell_type":"code","source":"augmenter = keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n        keras_cv.layers.RandomShear(\n            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n        ),\n        keras_cv.layers.JitteredResize(\n            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n        ),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.124552Z","iopub.status.idle":"2024-01-16T01:08:09.124965Z","shell.execute_reply.started":"2024-01-16T01:08:09.124758Z","shell.execute_reply":"2024-01-16T01:08:09.124783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Training Dataset","metadata":{}},{"cell_type":"code","source":"train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.shuffle(BATCH_SIZE * 4)\ntrain_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\ntrain_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.126422Z","iopub.status.idle":"2024-01-16T01:08:09.126842Z","shell.execute_reply.started":"2024-01-16T01:08:09.126641Z","shell.execute_reply":"2024-01-16T01:08:09.126659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Validation Dataset","metadata":{}},{"cell_type":"code","source":"resizing = keras_cv.layers.JitteredResize(\n    target_size=(640, 640),\n    scale_factor=(0.75, 1.3),\n    bounding_box_format=\"xyxy\",\n)\n\nval_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.shuffle(BATCH_SIZE * 4)\nval_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\nval_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.127986Z","iopub.status.idle":"2024-01-16T01:08:09.128367Z","shell.execute_reply.started":"2024-01-16T01:08:09.128178Z","shell.execute_reply":"2024-01-16T01:08:09.128195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n    inputs = next(iter(inputs.take(1)))\n    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=value_range,\n        rows=rows,\n        cols=cols,\n        y_true=bounding_boxes,\n        scale=5,\n        font_scale=0.7,\n        bounding_box_format=bounding_box_format,\n        class_mapping=class_mapping,\n    )\n\n\nvisualize_dataset(\n    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n)\n\nvisualize_dataset(\n    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.130204Z","iopub.status.idle":"2024-01-16T01:08:09.130772Z","shell.execute_reply.started":"2024-01-16T01:08:09.130471Z","shell.execute_reply":"2024-01-16T01:08:09.130518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_to_tuple(inputs):\n    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n\n\ntrain_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n\nval_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.132037Z","iopub.status.idle":"2024-01-16T01:08:09.132624Z","shell.execute_reply.started":"2024-01-16T01:08:09.132297Z","shell.execute_reply":"2024-01-16T01:08:09.132334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Model","metadata":{}},{"cell_type":"code","source":"backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.134232Z","iopub.status.idle":"2024-01-16T01:08:09.134822Z","shell.execute_reply.started":"2024-01-16T01:08:09.134518Z","shell.execute_reply":"2024-01-16T01:08:09.134554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yolo = keras_cv.models.YOLOV8Detector(\n    num_classes=len(class_mapping),\n    bounding_box_format=\"xyxy\",\n    backbone=backbone,\n    fpn_depth=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.136823Z","iopub.status.idle":"2024-01-16T01:08:09.13739Z","shell.execute_reply.started":"2024-01-16T01:08:09.13709Z","shell.execute_reply":"2024-01-16T01:08:09.137115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile the Model","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(\n    learning_rate=LEARNING_RATE,\n    global_clipnorm=GLOBAL_CLIPNORM,\n)\n\nyolo.compile(\n    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.13902Z","iopub.status.idle":"2024-01-16T01:08:09.139615Z","shell.execute_reply.started":"2024-01-16T01:08:09.139289Z","shell.execute_reply":"2024-01-16T01:08:09.139314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## COCO Metric Callback","metadata":{}},{"cell_type":"code","source":"class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n    def __init__(self, data, save_path):\n        super().__init__()\n        self.data = data\n        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n            bounding_box_format=\"xyxy\",\n            evaluate_freq=1e9,\n        )\n\n        self.save_path = save_path\n        self.best_map = -1.0\n\n    def on_epoch_end(self, epoch, logs):\n        self.metrics.reset_state()\n        for batch in self.data:\n            images, y_true = batch[0], batch[1]\n            y_pred = self.model.predict(images, verbose=0)\n            self.metrics.update_state(y_true, y_pred)\n\n        metrics = self.metrics.result(force=True)\n        logs.update(metrics)\n\n        current_map = metrics[\"MaP\"]\n        if current_map > self.best_map:\n            self.best_map = current_map\n            self.model.save(self.save_path)  # Save the model when mAP improves\n\n        return logs","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.140687Z","iopub.status.idle":"2024-01-16T01:08:09.142601Z","shell.execute_reply.started":"2024-01-16T01:08:09.142333Z","shell.execute_reply":"2024-01-16T01:08:09.142358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model","metadata":{}},{"cell_type":"code","source":"yolo.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=3,\n    callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model.h5\")],\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.143965Z","iopub.status.idle":"2024-01-16T01:08:09.144378Z","shell.execute_reply.started":"2024-01-16T01:08:09.144182Z","shell.execute_reply":"2024-01-16T01:08:09.144201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Predictions","metadata":{}},{"cell_type":"code","source":"def visualize_detections(model, dataset, bounding_box_format):\n    images, y_true = next(iter(dataset.take(1)))\n    y_pred = model.predict(images)\n    y_pred = bounding_box.to_ragged(y_pred)\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=(0, 255),\n        bounding_box_format=bounding_box_format,\n        y_true=y_true,\n        y_pred=y_pred,\n        scale=4,\n        rows=2,\n        cols=2,\n        show=True,\n        font_scale=0.7,\n        class_mapping=class_mapping,\n    )\n\n\nvisualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")","metadata":{"execution":{"iopub.status.busy":"2024-01-16T01:08:09.149051Z","iopub.status.idle":"2024-01-16T01:08:09.149585Z","shell.execute_reply.started":"2024-01-16T01:08:09.149311Z","shell.execute_reply":"2024-01-16T01:08:09.149332Z"},"trusted":true},"execution_count":null,"outputs":[]}]}